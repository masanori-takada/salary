import pandas as pd
import pickle
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score, train_test_split
from lightgbm import LGBMRegressor
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

print("ğŸš€ ç²¾åº¦æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™...")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
df = pd.read_csv('./data/dataset/train_data.csv')
print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {df.shape}")

# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
print("ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
df = df.dropna()
df = df.reset_index(drop=True)

# ã‚¨ãƒªã‚¢ã®ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
le = LabelEncoder()
df['area'] = le.fit_transform(df['area'])

print("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸­...")

# æ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½œæˆ
df['age_position_ratio'] = df['age'] / (df['position'] + 1)  # å¹´é½¢ã¨è·ä½ã®æ¯”ç‡
df['experience_ratio'] = df['service_length'] / df['age']    # å‹¤ç¶šå¹´æ•°ã¨å¹´é½¢ã®æ¯”ç‡
df['study_overtime_ratio'] = df['study_time'] / (df['overtime'] + 1)  # å‹‰å¼·æ™‚é–“ã¨æ®‹æ¥­æ™‚é–“ã®æ¯”ç‡
df['total_time'] = df['commute'] + df['overtime']            # ç·åŠ´åƒé–¢é€£æ™‚é–“
df['family_burden'] = df['partner'] + df['num_child']        # å®¶æ—è² æ‹…

# å¹´é½¢ã‚«ãƒ†ã‚´ãƒª
df['age_category'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 55, 100], labels=[1, 2, 3, 4, 5])
df['age_category'] = df['age_category'].astype(int)

print("âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†")

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
feature_columns = [col for col in df.columns if col not in ['salary']]
X = df[feature_columns].values
y = df['salary'].values

print(f"âœ… æ”¹è‰¯ç‰ˆç‰¹å¾´é‡: {X.shape}, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {y.shape}")

# ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ãƒ»æ¤œè¨¼ç”¨ã«åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("ğŸ¤– è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½æ¯”è¼ƒä¸­...")

# ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
models = {
    'LightGBM': LGBMRegressor(
        n_estimators=1000,
        max_depth=20,
        num_leaves=31,
        min_child_weight=15,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbose=-1
    ),
    'XGBoost': xgb.XGBRegressor(
        n_estimators=1000,
        max_depth=15,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbosity=0
    ),
    'RandomForest': RandomForestRegressor(
        n_estimators=500,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    ),
    'GradientBoosting': GradientBoostingRegressor(
        n_estimators=500,
        max_depth=10,
        learning_rate=0.1,
        subsample=0.8,
        random_state=42
    )
}

best_model = None
best_score = float('inf')
best_model_name = ""

for name, model in models.items():
    print(f"ğŸ”„ {name} ã‚’è¨“ç·´ä¸­...")
    
    # è¨“ç·´
    model.fit(X_train, y_train)
    
    # äºˆæ¸¬
    y_pred = model.predict(X_test)
    
    # è©•ä¾¡
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    
    print(f"ğŸ“Š {name}: MAE={mae:.2f}, RMSE={rmse:.2f}, RÂ²={r2:.4f}")
    
    if mae < best_score:
        best_score = mae
        best_model = model
        best_model_name = name

print(f"ğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name} (MAE: {best_score:.2f})")

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ
print("ğŸ”® ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...")
ensemble_models = [models['LightGBM'], models['XGBoost'], models['RandomForest']]

# å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å–å¾—
predictions = []
for model in ensemble_models:
    pred = model.predict(X_test)
    predictions.append(pred)

# å¹³å‡ã‚’å–ã‚‹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰
ensemble_pred = np.mean(predictions, axis=0)

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®è©•ä¾¡
ensemble_mae = mean_absolute_error(y_test, ensemble_pred)
ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))
ensemble_r2 = r2_score(y_test, ensemble_pred)

print(f"ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: MAE={ensemble_mae:.2f}, RMSE={ensemble_rmse:.2f}, RÂ²={ensemble_r2:.4f}")

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å†è¨“ç·´
print(f"ğŸ”„ {best_model_name} ã‚’ãƒ•ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å†è¨“ç·´ä¸­...")
final_model = type(best_model)(**best_model.get_params())
final_model.fit(X, y)

# ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬
sample_pred = final_model.predict(X[:5])
print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬: {sample_pred}")

# æ”¹è‰¯ç‰ˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿å­˜ï¼ˆæ–°ã—ã„ç‰¹å¾´é‡å«ã‚€ï¼‰
print("ğŸ’¾ æ”¹è‰¯ç‰ˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿å­˜ä¸­...")
df.to_pickle('./data/df_improved.pkl')

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
print("ğŸ’¾ æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
with open("./data/model_improved.pkl", "wb") as f:
    pickle.dump(final_model, f)

# ç‰¹å¾´é‡åã‚‚ä¿å­˜
with open("./data/feature_columns.pkl", "wb") as f:
    pickle.dump(feature_columns, f)

print("ğŸ‰ ç²¾åº¦æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ï¼")
print(f"ğŸ“ˆ ç²¾åº¦å‘ä¸Š: {best_model_name}ã‚’æ¡ç”¨")
print("ğŸ”„ Flaskã‚¢ãƒ—ãƒªã‚’æ”¹è‰¯ç‰ˆã«æ›´æ–°ã—ã¦ãã ã•ã„") 